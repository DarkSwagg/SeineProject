{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should be revised because some stations appear many times. Is it because we remove the firt \"0\"?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different types of graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary functions from other files\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# to facilitate the use of notebooks\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Get the current directory of the notebook\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "functions_dir = os.path.join(parent_dir, \"common\")\n",
    "print(functions_dir)\n",
    "\n",
    "# Add this directory to sys.path\n",
    "if functions_dir not in sys.path:\n",
    "    sys.path.append(functions_dir)\n",
    "\n",
    "from merge_metadata import add_HS, add_coord"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current directory of the notebook\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir, os.pardir))\n",
    "# Import phychem data\n",
    "param_file = os.path.join(parent_dir, \"phychem_data\", \"stations\", \"mostfreq_sta.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the list from the file\n",
    "mostfreq_sta = []\n",
    "with open(param_file, 'r') as file:\n",
    "    for line in file:\n",
    "        # Convert each line to the appropriate type\n",
    "        mostfreq_sta.append(str(line.strip()))\n",
    "\n",
    "len(mostfreq_sta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read excel files\n",
    "\n",
    "xlsx_names = os.listdir(os.path.join(os.getcwd(),\"phychem_param_80_19\", \"param_station_xlsx\"))\n",
    "# get stations ID from name of file\n",
    "list_station_ID = []\n",
    "# store df to create a final one\n",
    "list_df_xlsx = []\n",
    "\n",
    "for xlsx_name in tqdm(xlsx_names):\n",
    "\n",
    "    station_ID = xlsx_name.split(\"_\")[1].split(\".\")[0]\n",
    "    \n",
    "    if station_ID.startswith(\"0\"):\n",
    "        station_ID = station_ID.split(\"0\",1)[1]\n",
    "\n",
    "    if station_ID in list_station_ID:\n",
    "        continue\n",
    "        # print(f\"station was already added: {station_ID}\")\n",
    "    else:\n",
    "        list_station_ID.append(station_ID)\n",
    "        df = pd.read_excel(os.path.join(os.getcwd(),\"phychem_param_80_19\",\"param_station_xlsx\", xlsx_name))\n",
    "        list_df_xlsx.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 0 in first position in station ID\n",
    "\n",
    "for i,ID in enumerate(list_station_ID):\n",
    "    if ID.startswith('0'):\n",
    "        ID = ID.lstrip('0')\n",
    "        list_station_ID.pop(i)\n",
    "        list_station_ID.insert(i, ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store concentration values in one df per station\n",
    "\n",
    "name_cols = [\"station_ID\"] # already in english\n",
    "years = []\n",
    "seasons = []\n",
    "\n",
    "concentration_df = pd.DataFrame()\n",
    "\n",
    "for index, df_xlsx in tqdm(enumerate(list_df_xlsx),total=len(list_df_xlsx)): \n",
    "\n",
    "    if index==0: # get only first time\n",
    "        years += df_xlsx[\"Year\"].tolist()\n",
    "        seasons += df_xlsx[\"Season\"].tolist()\n",
    "        name_cols += df_xlsx.columns[2:].tolist()\n",
    "        concentration_df = pd.DataFrame(columns=name_cols)\n",
    "\n",
    "    df2 = pd.DataFrame(columns=name_cols[1::])\n",
    "    df2.loc[0] = [[] for i in range(len(name_cols[1::]))]\n",
    "\n",
    "    med_per_col = []\n",
    "    for col in name_cols[1::]: # neglect first col of station_ID)\n",
    "        df2.at[0,col] = df_xlsx[col].tolist()\n",
    "\n",
    "    \n",
    "    df2.insert(loc=0, column=\"station_ID\", value=list_station_ID[index])\n",
    "\n",
    "    concentration_df = pd.concat([concentration_df, df2], ignore_index=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all index corresponding to each season. \n",
    "seas_indx = {seas: np.where(np.array(seasons)==seas)[0].tolist() for seas in np.unique(seasons)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MF stations\n",
    "freq_conc_df = concentration_df[concentration_df[\"station_ID\"].isin(mostfreq_sta)].reset_index(drop=True)\n",
    "freq_conc_df.shape, freq_conc_df[\"station_ID\"].unique().shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to del outliers from carbon values greater than 100 with np.nan\n",
    "def replace_outliers(value_list):\n",
    "    return [np.nan if (isinstance(x, (int, float)) and x > 100) else x for x in value_list]\n",
    "    \n",
    "concentration_df[\"Organic carbon\"] = concentration_df[\"Organic carbon\"].apply(replace_outliers)\n",
    "freq_conc_df[\"Organic carbon\"] = freq_conc_df[\"Organic carbon\"].apply(replace_outliers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concentration_df = add_HS(type=\"Physico-chemical\", left_df = concentration_df, left_id=\"station_ID\")\n",
    "freq_conc_df = add_HS(type=\"Physico-chemical\", left_df = freq_conc_df, left_id=\"station_ID\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get median with no nan\n",
    "def grouping_df(df, name_cols=name_cols):\n",
    "   \n",
    "    # transform -1 into nan\n",
    "    for col in name_cols[1:]:\n",
    "        df[col] = df[col].apply(lambda x : [np.nan if value == -1 else value for value in x ])\n",
    "\n",
    "    # create functions stored in a dict\n",
    "    values = [list] + [lambda x: np.nanmedian(np.stack(x, axis=0), axis=0) for i in range(len(name_cols[1:]))]\n",
    "    hs_dict = {key: value for key, value in zip(name_cols, values)}\n",
    "\n",
    "    # apply functions during groupby\n",
    "    hs_grouped_df = df.groupby(\"ORD_STRA\").agg(hs_dict).reset_index()\n",
    "\n",
    "    return hs_grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_grouped_conc, hs_grouped_freqconc = grouping_df(concentration_df), grouping_df(freq_conc_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "methode pour x ticks:\n",
    "recupreer pour chaque saison, premiere annee non nan\n",
    "derniere annee non nan\n",
    "faire un arange entre les deux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_conc(hs_grouped_df, stations=\"All stations\", name_cols=name_cols, seas=\"all\", seas_indx=seas_indx, years=years):\n",
    "\n",
    "    fig_row = len(name_cols[1:])//2\n",
    "    fig, ax = plt.subplots(fig_row, 2, figsize=(20,15), layout=\"tight\")\n",
    "\n",
    "    if seas!=\"all\":\n",
    "        index = seas_indx[seas]\n",
    "        years = np.array(years)[index] # take corresponding years (manage 1980 and 2019)\n",
    "            \n",
    "\n",
    "    for i, col_df in enumerate(name_cols[1:]):\n",
    "        row = i % fig_row\n",
    "        col = i // fig_row\n",
    "\n",
    "        for j, row_df in hs_grouped_df.iterrows():\n",
    "\n",
    "            if seas!=\"all\":\n",
    "                value = row_df[col_df][index]   \n",
    "            else:\n",
    "                value = row_df[col_df]\n",
    "                \n",
    "            ax[row][col].plot([i for i in range(len(value))], value, \"-o\", markersize=2, label=f\"HS {j+1}\")\n",
    "   \n",
    "\n",
    "        # xticks = ax[row][col].get_xticks()\n",
    "        # ax[row][col].set_xticks(xticks[1:-1], labels = [years[math.ceil(k)] for k in xticks[1:-1]]) # modify to accept first element xticks\n",
    "        ax[row][col].legend()\n",
    "        ax[row][col].set_title(f\"{col_df}\", fontsize=18)\n",
    "\n",
    "    if seas!=\"all\": title = f\"{stations} - Concentrations {seas} season 1980-2019\"\n",
    "    else: title = f\"{stations} - Concentrations all seasons 1980-2019\"\n",
    "    \n",
    "    fig.suptitle(title, fontsize=25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all stations\n",
    "graph_conc(hs_grouped_df=hs_grouped_conc, seas=\"WT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent stations\n",
    "graph_conc(hs_grouped_df=hs_grouped_freqconc, stations=\"Most frequent stations\", seas=\"WT\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find best trend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part should be revised"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        slope1 error1 #nan1 slope2 error2 #nan2\n",
    "1980\n",
    "1981\n",
    "80-82 ; 83-19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = seas_indx[\"SM\"]\n",
    "\n",
    "values = hs_grouped_conc.at[5, \"Dissolved oxygen\"]\n",
    "SM_val = np.array(values[index])\n",
    "year_unique = np.unique(years).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_seg_error(data, split_index):\n",
    "    # Diviser les données en deux segments\n",
    "    segment1 = data[:split_index+1] # split index included\n",
    "    segment2 = data[split_index:] # split index included\n",
    "\n",
    "    X1 = np.where(~np.isnan(segment1))[0] # get index of those who are not nan\n",
    "    X2 = np.where(~np.isnan(segment2))[0]\n",
    "\n",
    "    nb_nan1 = len(segment1) - len(X1) # get number of nan\n",
    "    nb_nan2 = len(segment2) - len(X2)\n",
    "\n",
    "    # remove nan\n",
    "    segment1 = segment1[~np.isnan(segment1)] # get data without nan\n",
    "    segment2 = segment2[~np.isnan(segment2)]\n",
    "\n",
    "    # Ajuster une régression linéaire pour chaque segment\n",
    "    model1 = LinearRegression()\n",
    "    model2 = LinearRegression()\n",
    "    \n",
    "    model1.fit(X1.reshape(-1, 1), segment1.reshape(-1, 1))\n",
    "    model2.fit(X2.reshape(-1, 1), segment2.reshape(-1, 1))\n",
    "    \n",
    "    # Prédire les valeurs\n",
    "    predictions1 = model1.predict(X1.reshape(-1, 1))\n",
    "    predictions2 = model2.predict(X2.reshape(-1, 1))\n",
    "    \n",
    "    # Calculer la somme des erreurs au carré pour chaque segment\n",
    "    error1 = np.mean(np.abs(predictions1 - segment1.reshape(-1, 1)))\n",
    "    error2 = np.mean(np.abs(predictions2 - segment2.reshape(-1, 1)))\n",
    "\n",
    "    slope1 = model1.coef_[0]\n",
    "    slope2 = model2.coef_[0]\n",
    "\n",
    "    \n",
    "    return [slope1, error1, nb_nan1, slope2, error2, nb_nan2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_segmentation(value=SM_val, years=year_unique):\n",
    "\n",
    "    df = pd.DataFrame(columns= [\"seg_year\", \"slope1\", \"err1\", \"nb_nan1\", \"slope2\", \"err2\", \"nb_nan2\"])\n",
    "\n",
    "    for i, seg_year in enumerate(years[1:-1]): # traiter apres\n",
    "        \n",
    "        df2 = pd.DataFrame(columns= [\"seg_year\", \"slope1\", \"err1\", \"nb_nan1\", \"slope2\", \"err2\", \"nb_nan2\"]\n",
    "                           )\n",
    "        \n",
    "        # if i==0:\n",
    "        #     calculate_seg_error(value, i)\n",
    "\n",
    "        if np.isnan(value[i]):\n",
    "            result = [seg_year] + [np.nan for i in range(6)]\n",
    "\n",
    "        else:\n",
    "            result = [seg_year] + calculate_seg_error(value, i+1) # +1 changer apres\n",
    "            \n",
    "        print(result, i)\n",
    "\n",
    "        for j, col in enumerate(df2.columns):\n",
    "            df2[col] = result[j]\n",
    "\n",
    "        df = pd.concat((df, df2), ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_segmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(data):\n",
    "    min_error = float('inf')\n",
    "    best_split = None\n",
    "    \n",
    "    for i in range(1, len(data) - 1):\n",
    "        current_error = calculate_total_error(data, i)\n",
    "        if current_error < min_error:\n",
    "            min_error = current_error\n",
    "            best_split = i\n",
    "    \n",
    "    return best_split\n",
    "\n",
    "best_split_index = find_best_split(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en deux segments\n",
    "segment1 = data[:best_split_index]\n",
    "segment2 = data[best_split_index:]\n",
    "\n",
    "# Ajuster une régression linéaire pour chaque segment\n",
    "model1 = LinearRegression()\n",
    "model2 = LinearRegression()\n",
    "\n",
    "X1 = np.array(range(len(segment1))).reshape(-1, 1)\n",
    "X2 = np.array(range(len(segment2))).reshape(-1, 1)\n",
    "\n",
    "model1.fit(X1, segment1['concentration'])\n",
    "model2.fit(X2, segment2['concentration'])\n",
    "\n",
    "# Prédire les valeurs\n",
    "predictions1 = model1.predict(X1)\n",
    "predictions2 = model2.predict(X2)\n",
    "\n",
    "# Tracer les résultats\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index, data['concentration'], label='Data')\n",
    "plt.plot(segment1.index, predictions1, label='Segment 1 Trend', color='r')\n",
    "plt.plot(segment2.index, predictions2, label='Segment 2 Trend', color='g')\n",
    "plt.axvline(x=data.index[best_split_index], color='k', linestyle='--', label='Change Point')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Concentration de Nitrogen')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SeineProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
